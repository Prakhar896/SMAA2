{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![NYPLogo.png](attachment:NYPLogo.png)\n",
    "\n",
    "# Practical 11a: Extract Text Data\n",
    "\n",
    "## Objectives\n",
    "# * Demonstrate different methods of acquiring text data from different data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Data from PDFs\n",
    "Most of the time our data will be stored as PDF files. We need to extract text from these files and store it for further analysis. \n",
    "\n",
    "The simplest way to do this is by using the PyPDF2 libary. \n",
    "\n",
    "Follow the steps in this session to extract text data from PDF files.\n",
    "\n",
    "\n",
    "### Install and import all the necessary libraries\n",
    "To install the PyPDF2 library:\n",
    "> Type this command in jupyter or anaconda prompt: **pip install pypdf2**\n",
    "\n",
    "```Python\n",
    "# import libraries\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text from PDF file\n",
    "Download the **sample.pdf** file and placed in the same directory as your Jupyter notebook or Python script.\n",
    "\n",
    "Now we will extract the text using the Python code below.\n",
    "```Python\n",
    "# create a pdf file object and open the sample.pdf\n",
    "pdf = open(\"sample.pdf\", \"rb\")\n",
    "\n",
    "# create pdf reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "\n",
    "# check the number of pages in the pdf file\n",
    "print (pdf_reader.pages)\n",
    "\n",
    "# create a page object \n",
    "page = pdf_reader.pages[0]\n",
    "\n",
    "# extract the text from the page\n",
    "print (page.extract_text())\n",
    "\n",
    "# clode the pdf file\n",
    "pdf.close()\n",
    "```\n",
    "\n",
    "Note that the codes above does not work for scanned PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Download the **sample2.pdf** and try the following tasks:\n",
    "1. Read the all the pages in sample2.pdf and print out the extracted text\n",
    "\n",
    "<em>**Hint use loop to read all pages</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Data from Word Files\n",
    "Word files are one of the most common files that an organisation deals with. We need to extract text from these files and store it for further analysis. \n",
    "\n",
    "The simplest way to do this is by using the docx libary. \n",
    "\n",
    "Follow the steps in this session to extract text data from Word files.\n",
    "\n",
    "\n",
    "### Install and import all the necessary libraries\n",
    "To install the docx library:\n",
    "> Type this command in jupyter or anaconda prompt: **pip install python-docx**\n",
    "\n",
    "```Python\n",
    "# import library\n",
    "from docx import Document\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text from PDF file\n",
    "Download the **sample.docx** file and placed in the same directory as your Jupyter notebook or Python script.\n",
    "\n",
    "Now we will extract the text using the Python code below.\n",
    "```Python\n",
    "# create a word file object\n",
    "doc = open(\"sample.docx\", \"rb\")\n",
    "\n",
    "# create word reader object\n",
    "doc = Document(doc)\n",
    "\n",
    "# create an empty string and call this document. This document variable store each paragraph in the Word document.\n",
    "# We then create a for loop that goes through each paragraph in the Word document and appends the paragraph.\n",
    "docu=\"\"\n",
    "for para in doc.paragraphs:\n",
    "    docu += para.text\n",
    "\n",
    "# print the extract text\n",
    "print(docu)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text Data from RSS Feeds\n",
    "RSS (Rich Site Summary) is a format for delivering regularly change web content. Many news-related sites, weblogs and other online publishers syndicate their content as an RSS Feed to whoever wants it. \n",
    "\n",
    "The simplest way to do this is by using the feedparser libary. \n",
    "\n",
    "Follow the steps in this session to extract text data from RSS feeds.\n",
    "\n",
    "\n",
    "### Install and import all the necessary libraries\n",
    "To install the feedparser library:\n",
    "> Type this command in jupyter or anaconda prompt: **pip install feedparser**\n",
    "\n",
    "```Python\n",
    "# import library\n",
    "import feedparser\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Structure\n",
    "In the below example, we will get the structure of the feed so that we can analyse further about which parts of the feed want to process. Use the feedparser.parse() function for creating a feed object which contains parsed blog. It takes the URL of the blog feed.\n",
    "```Python\n",
    "# create feed and put in the RSS feed url\n",
    "news_feed = feedparser.parse(\"https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml&category=10416\")\n",
    "\n",
    "news_feed.keys()\n",
    "```\n",
    "Feed has some general information about the rss feed, but the \"meat\" of the feed is in entries. The rest of the keys weren't all that useful.\n",
    "```Python\n",
    "# get keys under entries\n",
    "news_feed.entries[0].keys()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Details of Feed\n",
    "RSS documents often know as feed which consists of text, and metadata, like time and author's name.\n",
    "```Python\n",
    "# returns title of rss feed\n",
    "print(news_feed.feed.title)\n",
    "\n",
    "# return link of rss feed and number of entries\n",
    "print(news_feed.feed.link)\n",
    "print(len(news_feed.entries))\n",
    "\n",
    "# details of individual entries can be accessed by using attribute nam\n",
    "print(news_feed.entries[0].title)\n",
    "print(news_feed.entries[0].link)\n",
    "print(news_feed.entries[0].published)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try the following tasks:\n",
    "1. Read this rss feed: https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml\n",
    "2. Save the entries title, link, published into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text Data from Twitter\n",
    "Twitter has a gigantic amount of data with a lot of value in it. Social media marketers are making their living from it. There is an enormous amount of tweets every day, and every tweet has some story to tell. When all of this data is collected and analysed, it gives a tremendous amount of insights to a business about their company, product, service, etc. \n",
    "\n",
    "In February 2023, Twitter set unrealistic prices for its API, giving away crumbs of data for big bucks. Some started using libraries such as snscrape, which used web public APIs. But in April 2023, Twitter closed that option as well â€” making search only for authorized accounts.\n",
    "\n",
    "In the exercise below to use Python to scrape data from Twitter. With Twitter's recent changes and restricting use of its API this will be a super useful method to allow you to extract narrow time data from Twitter. \n",
    "\n",
    "In order to scrape our data we are going to use the **scrapper API (decommissioned)**. To use API  you first have to create an account so you can go ahead and go to scrapperapi.com. Click on **Get Started for Free** to sign up for an account. After creating the account, it will give you an API key that you need in the code later. \n",
    "\n",
    "As Twitter.com became X.com it closed its public API though web scraping.\n",
    "However, some 3rd party APIs are available for us to still scrap data from X.com!\n",
    "\n",
    "In this X.com web scraping tutorial, we'll take a look at scraping X.com posts and profiles using Python and Playwright.\n",
    "Visit https://scrapfly.io/blog/how-to-scrape-twitter/ and following the instructions to scrap data off x.com.\n",
    "As an alternative to scrapping data from x.com, we could refer to https://github.com/scrapfly/scrapfly-scrapers/tree/main/twitter-scraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text Data from Twitter (Outdated)\n",
    "Twitter has a gigantic amount of data with a lot of value in it. Social media marketers are making their living from it. There is an enormous amount of tweets every day, and every tweet has some story to tell. When all of this data is collected and analysed, it gives a tremendous amount of insights to a business about their company, product, service, etc. \n",
    "\n",
    "In February 2023, Twitter set unrealistic prices for its API, giving away crumbs of data for big bucks. Some started using libraries such as snscrape, which used web public APIs. But in April 2023, Twitter closed that option as well â€” making search only for authorized accounts.\n",
    "\n",
    "In the exercise below to use Python to scrape data from Twitter. With Twitter's recent changes and restricting use of its API this will be a super useful method to allow you to extract narrow time data from Twitter. \n",
    "\n",
    "In order to scrape our data we are going to use the **scrapper API**. To use API  you first have to create an account so you can go ahead and go to scrapperapi.com. Click on **Get Started for Free** to sign up for an account. After creating the account, it will give you an API key that you need in the code later. \n",
    "\n",
    "### Install and import all the necessary libraries\n",
    "First we need to import two libraries. The first one is request which is what we will be using to make sure HTTP requests to actually go out to the web and scrape data. We'll also import pandas to get the data we scrape from twitter back into data frames.\n",
    "\n",
    "```Python\n",
    "# import the libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data and the keys. We can see that we have three keys: search_information, organic_results and pagination. From the organic_results, you can see there is title, and the snippet from the tweet and it gives you the link and the displayed link.\n",
    "\n",
    "If we look at the pagination, it give you some additional metadata about your data pool. The information that we wanted are in the organic_results.\n",
    "\n",
    "~~~Python\n",
    "print (data.keys())\n",
    "data['organic_results']\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the type that was returned for organic_results. We can see that it is a list so we can actually access certain elements of organic results. Each element within that list is a dictionary and we can use the keys to access more data. \n",
    "\n",
    "```Python\n",
    "# data type of organic_results\n",
    "print (type(data['organic_results']))\n",
    "\n",
    "# get first element of organic_results\n",
    "display(data['organic_results'][0])\n",
    "\n",
    "# get the title of the first tweet\n",
    "print (data['organic_results'][0]['title'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert our scrape data to a dataframe. First we need to iterate through the tweets in the organic_results and placed it in a twitter_data list. Using the list we will create a dataframe. \n",
    "\n",
    "~~~Python\n",
    "twitter_data = []\n",
    "for tweet in data['organic_results']:\n",
    "    twitter_data.append(tweet)\n",
    "    \n",
    "df = pd.DataFrame(twitter_data)\n",
    "df\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try the following tasks:\n",
    "1. Scrape 50 tweets from Twitter on \"**chatgpt**\"\n",
    "2. Save the scrape results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
